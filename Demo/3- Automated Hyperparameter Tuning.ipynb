{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the unique features of **NiftyTorch** is the ability to perform automated hyperparameter tuning on different CNN for neuroimaging data. Here is an example of such technique. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AlexNet example\n",
    "Here we use `niftytorch` automated hyperparameter optimization module to optimze [AlexNet](https://en.wikipedia.org/wiki/AlexNet). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the required libraries \n",
    "import torch\n",
    "import niftytorch\n",
    "from niftytorch.Models.AlexNet import train_alexnet\n",
    "from torchvision import transforms\n",
    "from niftytorch.Loss.Losses import FocalLoss\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.optim import lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define path to data and labels\n",
    "data_folder = \"/example/farshid/img/data/StudyName\"\n",
    "data_csv = \"/example/farshid/img/data/StudyName/label.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the `DataLoader` of the `niftytorch` is designed in way that no additional changes to the existing `torch` data input/output modules are required. Users will be able to still use their favorite `torch` commands, while `niftytorch` is taking care of the 3D adaptation in the background. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define tensor transformers and loss\n",
    "data_transforms = transforms.Compose([transforms.ToTensor()])\n",
    "train = train_alexnet()\n",
    "loss_list = [nn.CrossEntropyLoss()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration file\n",
    "Below cell shows an example of `cfgs` file, containing a list of configurations for AlexNet. For full description on the configuration, see the documentation files. \n",
    "\n",
    "Here we assume: \n",
    "* **label.csv** file contains a column `Subject` with the name of sample subjects.\n",
    "* **label.csv** file contains a column `diagnosis` with labels for each class (e.g. \"normal\",\"disease\").\n",
    "* **label.csv** file contains demographic informations, including `['age','gender','education']`. \n",
    "* Study data folder contains `t1w.nii.gz` and `flair.nii.gz` for all subjects. \n",
    "* For the purpose of the Demo we set the number of epochs and trials to 10 and 20, respectively. Consider higher numbers, specially if the computational resources are available. \n",
    "* Define the lower and upper bounds of the learning rate using `lr_min` and `lr_max`.\n",
    "\n",
    "The study folder should contain training (`train`) and validation (`val`) folders, organized as follow:\n",
    "```\n",
    "StudyName\n",
    "└───train\n",
    "│   └───subjectID\n",
    "│   │       t1w.nii.gz\n",
    "│   │       flair.nii.gz\n",
    "│   └───subjectID\n",
    "│   │       t1w.nii.gz\n",
    "│   │       flair.nii.gz              \n",
    "│   │   ...\n",
    "│   \n",
    "└───val\n",
    "│   └───subjectID\n",
    "│   │       t1w.nii.gz\n",
    "│   │       flair.nii.gz\n",
    "│   └───subjectID\n",
    "│   │       t1w.nii.gz\n",
    "│   │       flair.nii.gz             \n",
    "│   │   ...\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note on optimizer\n",
    "Users can choose available optimizers on Torch for their applications. Sometimes when a given optimizations do not progress, it is likely that the optimizer is not suited to your dataset. A list of optimizers are available in Torch which can be used, including but not limited to:\n",
    "```optim.ASGD, optim.Adam, optim.LBFGS, optim.Rprop, optim.AdamW, optim.SGD, optim.SparseAdam```\n",
    "\n",
    "With Automated hyperparameter optimization module you can choose a list of optimizer using:\n",
    "\n",
    "```'opt_list': [optim.Adam,optim.RMSprop]```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the configurations\n",
    "cfgs = {\n",
    "    'num_classes':2,\n",
    "    'in_channels':2,\n",
    "    'data_folder':data_folder,\n",
    "    'data_csv':data_csv,\n",
    "    'data_transforms':data_transforms,\n",
    "    'filename_label':'Subject',\n",
    "    'class_label':'diagnosis',\n",
    "    'channels':False,\n",
    "    'channels_1':[32, 96, 144, 8],\n",
    "    'channels_2':[32, 96, 144, 8],\n",
    "    'channels_3':[32, 96, 144, 8],\n",
    "    'channels_4':[32, 96, 144, 8],\n",
    "    'channels_5':[32, 96, 144, 8],\n",
    "    'l2':3e-4,\n",
    "    'strides':False,\n",
    "    'strides_1':[1],\n",
    "    'strides_2':[1], \n",
    "    'strides_3':[2, 1], \n",
    "    'strides_4':[2, 1], \n",
    "    'strides_5':[1],\n",
    "    'kernel_size':False,\n",
    "    'kernel_size_1':[5, 3],\n",
    "    'kernel_size_2':[5, 3], \n",
    "    'kernel_size_3':[5, 3], \n",
    "    'kernel_size_4':[5, 3], \n",
    "    'kernel_size_5':[3],\n",
    "    'padding':False,\n",
    "    'padding_1':[0, 1],\n",
    "    'padding_2':[0, 1],\n",
    "    'padding_3':[0, 2, 1],\n",
    "    'padding_4':[0, 2, 1],\n",
    "    'padding_5':[2, 1],\n",
    "    'learning_rate': False,\n",
    "    'lr_min':1e-6,\n",
    "    'lr_max':1e-2,\n",
    "    'loss':False,\n",
    "    'demographic':['age','gender','education'],\n",
    "    'loss_list':loss_list,\n",
    "    'scheduler':lr_scheduler.StepLR,\n",
    "    'image_scale':False,\n",
    "    'num_epochs':10,\n",
    "    'image_scale_list':[64,80],\n",
    "    'optimizer':False,\n",
    "    'opt_list': [optim.Adam,optim.RMSprop],\n",
    "    'gamma':.2,\n",
    "    'batch_size':8,\n",
    "    'num_workers':2,\n",
    "    'cuda':'cuda:2',\n",
    "    'device_ids':[],\n",
    "    'step_size':7,\n",
    "    'file_type':('t1w.nii.gz','flair.nii.gz')\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the `cfgs` are set, hyperparameter optimization can be done by running below code. Note that this step is highly computational demanding. We have tested it on a GPU server, but may be slow on local machines. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.hyperopt_set_params(cfgs)\n",
    "train.optimize(n_trials = 2,\n",
    "    contour_plot_params = ['image_size','lr'],\n",
    "    optimization_history = True,\n",
    "    plot_parallel_coordinate_params = ['image_size','lr'],\n",
    "    slice_plot_params = ['image_size','lr'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above example executes **20 trials**, set by `n_trials = 20` option. \n",
    "\n",
    "At the end of each trial an update of the internal progress will be printed. For example: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> ```\n",
    "Finished trial#0 with value: 0.7536231884057971 with parameters: {'lr': 0.00018164197161668776, 'optimizer': <class 'torch.optim.rmsprop.RMSprop'>, 'loss': CrossEntropyLoss(), 'image_size': 80, 'channels_1': 8, 'channels_2': 144, 'channels_3': 96, 'channels_4': 96, 'channels_5': 96, 'strides_1': 1, 'strides_2': 1, 'strides_3': 2, 'strides_4': 2, 'strides_5': 1, 'kernel_size_1': 3, 'kernel_size_2': 3, 'kernel_size_3': 5, 'kernel_size_4': 3, 'kernel_size_5': 3, 'padding_1': 1, 'padding_2': 0, 'padding_3': 2, 'padding_4': 0, 'padding_5': 2}. Best is trial#0 with value: 0.7536231884057971.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When hyperparameter tuning is completed, the optimized hyperparamters of the network are obtained. \n",
    "\n",
    "In addition, we save optimization history. Above code outputs a file called `results.pkl` in the working directory in which the code was run. The `pkl` file contains performace results for each run and can be accessed using below commands: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number</th>\n",
       "      <th>value</th>\n",
       "      <th>datetime_start</th>\n",
       "      <th>datetime_complete</th>\n",
       "      <th>duration</th>\n",
       "      <th>params_channels_1</th>\n",
       "      <th>params_channels_2</th>\n",
       "      <th>params_channels_3</th>\n",
       "      <th>params_channels_4</th>\n",
       "      <th>params_channels_5</th>\n",
       "      <th>...</th>\n",
       "      <th>params_padding_2</th>\n",
       "      <th>params_padding_3</th>\n",
       "      <th>params_padding_4</th>\n",
       "      <th>params_padding_5</th>\n",
       "      <th>params_strides_1</th>\n",
       "      <th>params_strides_2</th>\n",
       "      <th>params_strides_3</th>\n",
       "      <th>params_strides_4</th>\n",
       "      <th>params_strides_5</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.445135</td>\n",
       "      <td>2020-05-18 00:54:04.486563</td>\n",
       "      <td>2020-05-18 01:11:33.480228</td>\n",
       "      <td>00:17:28.993665</td>\n",
       "      <td>96</td>\n",
       "      <td>144</td>\n",
       "      <td>32</td>\n",
       "      <td>144</td>\n",
       "      <td>96</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   number     value             datetime_start          datetime_complete  \\\n",
       "0       0  0.445135 2020-05-18 00:54:04.486563 2020-05-18 01:11:33.480228   \n",
       "\n",
       "         duration  params_channels_1  params_channels_2  params_channels_3  \\\n",
       "0 00:17:28.993665                 96                144                 32   \n",
       "\n",
       "   params_channels_4  params_channels_5  ...  params_padding_2  \\\n",
       "0                144                 96  ...                 0   \n",
       "\n",
       "   params_padding_3  params_padding_4  params_padding_5  params_strides_1  \\\n",
       "0                 2                 1                 1                 1   \n",
       "\n",
       "   params_strides_2 params_strides_3  params_strides_4 params_strides_5  \\\n",
       "0                 1                2                 2                1   \n",
       "\n",
       "      state  \n",
       "0  COMPLETE  \n",
       "\n",
       "[1 rows x 30 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "restuls = joblib.load('results.pkl')\n",
    "df = restuls.trials_dataframe()\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(df.learning_rate,df.accuracy)\n",
    "# plt.xlabel('learning_rate')\n",
    "# plt.ylabel('accuracy')\n",
    "# plt.grid()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
